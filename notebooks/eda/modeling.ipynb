{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf984cc-1c05-4c6c-91b4-45c8447376b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
    "!pip3 install torch torchaudio torchvision torchtext torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372261a1-35e9-49de-8f39-b5040488fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46ae1e1-710f-43bf-b653-c4db1c930470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PYTORCH MODEL: Matrix Factorization with Biases\n",
    "# ============================================================================\n",
    "\n",
    "class MatrixFactorizationWithBias(nn.Module):\n",
    "    \"\"\"\n",
    "    Matrix Factorization model with user and item biases.\n",
    "    Learns latent factors to predict user-item ratings.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_items, embedding_dim=50):\n",
    "        super(MatrixFactorizationWithBias, self).__init__()\n",
    "        \n",
    "        # User and item embeddings (latent factors)\n",
    "        self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # User and item biases\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        \n",
    "        # Global bias (overall average)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # Initialize weights for better convergence\n",
    "        self.user_embeddings.weight.data.uniform_(0, 0.05)\n",
    "        self.item_embeddings.weight.data.uniform_(0, 0.05)\n",
    "        self.user_bias.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.item_bias.weight.data.uniform_(-0.01, 0.01)\n",
    "    \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        \"\"\"\n",
    "        Forward pass: compute predicted ratings\n",
    "        \n",
    "        Args:\n",
    "            user_ids: Tensor of user indices\n",
    "            item_ids: Tensor of item indices\n",
    "            \n",
    "        Returns:\n",
    "            Predicted ratings\n",
    "        \"\"\"\n",
    "        # Get latent vectors\n",
    "        user_vecs = self.user_embeddings(user_ids)\n",
    "        item_vecs = self.item_embeddings(item_ids)\n",
    "        \n",
    "        # Dot product of latent factors\n",
    "        dot_product = (user_vecs * item_vecs).sum(dim=1)\n",
    "        \n",
    "        # Add biases\n",
    "        user_b = self.user_bias(user_ids).squeeze()\n",
    "        item_b = self.item_bias(item_ids).squeeze()\n",
    "        \n",
    "        # Final prediction: dot product + biases + global bias\n",
    "        prediction = dot_product + user_b + item_b + self.global_bias\n",
    "        \n",
    "        return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f91028-c444-4a9b-bbaf-3cca5d23a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class MovieRatingsDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for movie ratings\"\"\"\n",
    "    def __init__(self, user_ids, item_ids, ratings):\n",
    "        self.user_ids = torch.LongTensor(user_ids.values)\n",
    "        self.item_ids = torch.LongTensor(item_ids.values)\n",
    "        self.ratings = torch.FloatTensor(ratings.values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.item_ids[idx], self.ratings[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924a966c-16cf-463c-abdf-27ccbf8e5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MovieRecommender:\n",
    "    \"\"\"\n",
    "    Complete recommendation system that handles:\n",
    "    - Data loading and preprocessing\n",
    "    - Model training with 80-20 split\n",
    "    - Top-3 movie recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ratings_path, movies_path=None, embedding_dim=50):\n",
    "        \"\"\"\n",
    "        Initialize the recommender system\n",
    "        \n",
    "        Args:\n",
    "            ratings_path: Path to ratings.csv (userId, movieId, rating, timestamp)\n",
    "            movies_path: Path to movies_encoded.csv (optional, for movie metadata)\n",
    "            embedding_dim: Size of latent feature vectors\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"INITIALIZING MOVIE RECOMMENDATION SYSTEM\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Load ratings data\n",
    "        print(f\"\\nLoading ratings from: {ratings_path}\")\n",
    "        self.ratings_df = pd.read_csv(ratings_path)\n",
    "        print(f\"‚úì Loaded {len(self.ratings_df):,} ratings\")\n",
    "        \n",
    "        # Load movie information if available\n",
    "        self.has_movie_info = False\n",
    "        if movies_path:\n",
    "            try:\n",
    "                self.movies_df = pd.read_csv(movies_path)\n",
    "                self.has_movie_info = True\n",
    "                print(f\"‚úì Loaded {len(self.movies_df):,} movies with metadata\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"‚ö† Could not load {movies_path}, will use movie IDs only\")\n",
    "        \n",
    "        # Create user and item mappings\n",
    "        self._create_mappings()\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = None\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        \n",
    "    def _create_mappings(self):\n",
    "        \"\"\"Create mappings between original IDs and model indices\"\"\"\n",
    "        # Get unique users and items\n",
    "        unique_users = sorted(self.ratings_df['userId'].unique())\n",
    "        unique_items = sorted(self.ratings_df['movieId'].unique())\n",
    "        \n",
    "        # Create bidirectional mappings\n",
    "        self.user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
    "        self.idx_to_user = {idx: user for user, idx in self.user_to_idx.items()}\n",
    "        \n",
    "        self.item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
    "        self.idx_to_item = {idx: item for item, idx in self.item_to_idx.items()}\n",
    "        \n",
    "        # Add mapped columns to dataframe\n",
    "        self.ratings_df['user_idx'] = self.ratings_df['userId'].map(self.user_to_idx)\n",
    "        self.ratings_df['item_idx'] = self.ratings_df['movieId'].map(self.item_to_idx)\n",
    "        \n",
    "        self.num_users = len(unique_users)\n",
    "        self.num_items = len(unique_items)\n",
    "        \n",
    "        print(f\"\\nüìä Dataset Statistics:\")\n",
    "        print(f\"   ‚Ä¢ Users: {self.num_users:,}\")\n",
    "        print(f\"   ‚Ä¢ Movies: {self.num_items:,}\")\n",
    "        print(f\"   ‚Ä¢ Ratings: {len(self.ratings_df):,}\")\n",
    "        print(f\"   ‚Ä¢ Sparsity: {100 * (1 - len(self.ratings_df) / (self.num_users * self.num_items)):.2f}%\")\n",
    "    \n",
    "    def prepare_data(self, test_size=0.2, batch_size=128, random_state=42):\n",
    "        \"\"\"\n",
    "        Split data into 80-20 train-test and create DataLoaders\n",
    "        \n",
    "        Args:\n",
    "            test_size: Fraction for testing (0.2 = 20%)\n",
    "            batch_size: Batch size for training\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        # 80-20 train-test split\n",
    "        train_df, test_df = train_test_split(\n",
    "            self.ratings_df,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìÇ Data Split (80-20):\")\n",
    "        print(f\"   ‚Ä¢ Training set: {len(train_df):,} ratings ({100*(1-test_size):.0f}%)\")\n",
    "        print(f\"   ‚Ä¢ Test set: {len(test_df):,} ratings ({100*test_size:.0f}%)\")\n",
    "        \n",
    "        # Create PyTorch datasets\n",
    "        train_dataset = MovieRatingsDataset(\n",
    "            train_df['user_idx'],\n",
    "            train_df['item_idx'],\n",
    "            train_df['rating']\n",
    "        )\n",
    "        \n",
    "        test_dataset = MovieRatingsDataset(\n",
    "            test_df['user_idx'],\n",
    "            test_df['item_idx'],\n",
    "            test_df['rating']\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        \n",
    "        # Store rating statistics\n",
    "        self.mean_rating = train_df['rating'].mean()\n",
    "        self.std_rating = train_df['rating'].std()\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Batch size: {batch_size}\")\n",
    "        print(f\"   ‚Ä¢ Mean rating: {self.mean_rating:.2f} ¬± {self.std_rating:.2f}\")\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Initialize the matrix factorization model\"\"\"\n",
    "        self.model = MatrixFactorizationWithBias(\n",
    "            num_users=self.num_users,\n",
    "            num_items=self.num_items,\n",
    "            embedding_dim=self.embedding_dim\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Set global bias to mean rating\n",
    "        self.model.global_bias.data.fill_(self.mean_rating)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"\\nüß† Model Architecture:\")\n",
    "        print(f\"   ‚Ä¢ Embedding dimension: {self.embedding_dim}\")\n",
    "        print(f\"   ‚Ä¢ Total parameters: {total_params:,}\")\n",
    "        print(f\"   ‚Ä¢ Device: {self.device}\")\n",
    "    \n",
    "    def train(self, epochs=20, lr=0.01, weight_decay=1e-5, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the recommendation model\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            lr: Learning rate\n",
    "            weight_decay: L2 regularization\n",
    "            verbose: Print training progress\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        print(f\"\\nüöÄ Training started...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        best_test_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # TRAINING PHASE\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for user_ids, item_ids, ratings in self.train_loader:\n",
    "                user_ids = user_ids.to(self.device)\n",
    "                item_ids = item_ids.to(self.device)\n",
    "                ratings = ratings.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = self.model(user_ids, item_ids)\n",
    "                loss = criterion(predictions, ratings)\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(self.train_loader)\n",
    "            \n",
    "            # EVALUATION PHASE\n",
    "            self.model.eval()\n",
    "            test_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for user_ids, item_ids, ratings in self.test_loader:\n",
    "                    user_ids = user_ids.to(self.device)\n",
    "                    item_ids = item_ids.to(self.device)\n",
    "                    ratings = ratings.to(self.device)\n",
    "                    \n",
    "                    predictions = self.model(user_ids, item_ids)\n",
    "                    loss = criterion(predictions, ratings)\n",
    "                    test_loss += loss.item()\n",
    "            \n",
    "            test_loss /= len(self.test_loader)\n",
    "            \n",
    "            # Store losses\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.test_losses.append(test_loss)\n",
    "            \n",
    "            # Track best model\n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "                best_epoch = epoch + 1\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and ((epoch + 1) % 5 == 0 or epoch == 0):\n",
    "                print(f\"Epoch [{epoch+1:2d}/{epochs}] | \"\n",
    "                      f\"Train Loss: {train_loss:.4f} | \"\n",
    "                      f\"Test Loss: {test_loss:.4f} | \"\n",
    "                      f\"RMSE: {np.sqrt(test_loss):.4f}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"‚úì Training complete!\")\n",
    "        print(f\"   ‚Ä¢ Best test RMSE: {np.sqrt(best_test_loss):.4f} (epoch {best_epoch})\")\n",
    "        print(f\"   ‚Ä¢ Final train RMSE: {np.sqrt(self.train_losses[-1]):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Final test RMSE: {np.sqrt(self.test_losses[-1]):.4f}\")\n",
    "    \n",
    "    def predict(self, user_id, movie_id):\n",
    "        \"\"\"\n",
    "        Predict rating for a user-movie pair\n",
    "        \n",
    "        Args:\n",
    "            user_id: Original user ID\n",
    "            movie_id: Original movie ID\n",
    "            \n",
    "        Returns:\n",
    "            Predicted rating (or None if user/movie not in training data)\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_to_idx or movie_id not in self.item_to_idx:\n",
    "            return None\n",
    "        \n",
    "        user_idx = self.user_to_idx[user_id]\n",
    "        item_idx = self.item_to_idx[movie_id]\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.LongTensor([user_idx]).to(self.device)\n",
    "            item_tensor = torch.LongTensor([item_idx]).to(self.device)\n",
    "            prediction = self.model(user_tensor, item_tensor)\n",
    "            \n",
    "        return prediction.item()\n",
    "    \n",
    "    def recommend_top_3(self, user_id, exclude_rated=True):\n",
    "        \"\"\"\n",
    "        Get top 3 movie recommendations for a user\n",
    "        \n",
    "        Args:\n",
    "            user_id: Original user ID\n",
    "            exclude_rated: Whether to exclude movies user has already rated\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with top 3 recommendations\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_to_idx:\n",
    "            print(f\"‚ùå User {user_id} not found in training data\")\n",
    "            return None\n",
    "        \n",
    "        user_idx = self.user_to_idx[user_id]\n",
    "        \n",
    "        # Get movies already rated by the user\n",
    "        if exclude_rated:\n",
    "            rated_movies = set(\n",
    "                self.ratings_df[self.ratings_df['userId'] == user_id]['movieId'].values\n",
    "            )\n",
    "        else:\n",
    "            rated_movies = set()\n",
    "        \n",
    "        # Get all available movies\n",
    "        all_movie_ids = list(self.item_to_idx.keys())\n",
    "        \n",
    "        # Predict ratings for all unrated movies\n",
    "        predictions = []\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Batch prediction for efficiency\n",
    "            unrated_movies = [m for m in all_movie_ids if m not in rated_movies]\n",
    "            \n",
    "            for movie_id in unrated_movies:\n",
    "                movie_idx = self.item_to_idx[movie_id]\n",
    "                \n",
    "                user_tensor = torch.LongTensor([user_idx]).to(self.device)\n",
    "                item_tensor = torch.LongTensor([movie_idx]).to(self.device)\n",
    "                \n",
    "                pred_rating = self.model(user_tensor, item_tensor).item()\n",
    "                predictions.append((movie_id, pred_rating))\n",
    "        \n",
    "        # Sort by predicted rating (descending)\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top 3\n",
    "        top_3 = predictions[:3]\n",
    "        \n",
    "        # Create result DataFrame\n",
    "        results = []\n",
    "        for rank, (movie_id, pred_rating) in enumerate(top_3, 1):\n",
    "            result = {\n",
    "                'rank': rank,\n",
    "                'movieId': int(movie_id),\n",
    "                'predicted_rating': round(pred_rating, 2)\n",
    "            }\n",
    "            \n",
    "            # Add movie metadata if available\n",
    "            if self.has_movie_info:\n",
    "                movie_info = self.movies_df[self.movies_df['movieId'] == movie_id]\n",
    "                if not movie_info.empty:\n",
    "                    # Add available columns\n",
    "                    for col in self.movies_df.columns:\n",
    "                        if col != 'movieId' and col in movie_info.columns:\n",
    "                            result[col] = movie_info[col].values[0]\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def get_user_profile(self, user_id):\n",
    "        \"\"\"Get user's rating statistics and profile\"\"\"\n",
    "        user_ratings = self.ratings_df[self.ratings_df['userId'] == user_id]\n",
    "        \n",
    "        if len(user_ratings) == 0:\n",
    "            return None\n",
    "        \n",
    "        profile = {\n",
    "            'user_id': user_id,\n",
    "            'num_ratings': len(user_ratings),\n",
    "            'mean_rating': user_ratings['rating'].mean(),\n",
    "            'min_rating': user_ratings['rating'].min(),\n",
    "            'max_rating': user_ratings['rating'].max(),\n",
    "            'std_rating': user_ratings['rating'].std()\n",
    "        }\n",
    "        \n",
    "        return profile\n",
    "    \n",
    "    def save_model(self, path='movie_recommender_model.pth'):\n",
    "        \"\"\"Save trained model and mappings\"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'user_to_idx': self.user_to_idx,\n",
    "            'item_to_idx': self.item_to_idx,\n",
    "            'num_users': self.num_users,\n",
    "            'num_items': self.num_items,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'mean_rating': self.mean_rating,\n",
    "            'train_losses': self.train_losses,\n",
    "            'test_losses': self.test_losses\n",
    "        }, path)\n",
    "        print(f\"\\nüíæ Model saved to: {path}\")\n",
    "    \n",
    "    def load_model(self, path='movie_recommender_model.pth'):\n",
    "        \"\"\"Load a pre-trained model\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        \n",
    "        self.user_to_idx = checkpoint['user_to_idx']\n",
    "        self.item_to_idx = checkpoint['item_to_idx']\n",
    "        self.num_users = checkpoint['num_users']\n",
    "        self.num_items = checkpoint['num_items']\n",
    "        self.embedding_dim = checkpoint['embedding_dim']\n",
    "        self.mean_rating = checkpoint['mean_rating']\n",
    "        \n",
    "        # Rebuild model\n",
    "        self.build_model()\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(f\"‚úì Model loaded from: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18a2fd9-70c9-4f0a-b64a-5a658d41c0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                    PYTORCH MOVIE RECOMMENDATION SYSTEM\n",
      "================================================================================\n",
      "================================================================================\n",
      "INITIALIZING MOVIE RECOMMENDATION SYSTEM\n",
      "================================================================================\n",
      "\n",
      "Loading ratings from: dataset/ratings.csv\n",
      "‚úì Loaded 25,000,095 ratings\n",
      "‚úì Loaded 62,423 movies with metadata\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "   ‚Ä¢ Users: 162,541\n",
      "   ‚Ä¢ Movies: 59,047\n",
      "   ‚Ä¢ Ratings: 25,000,095\n",
      "   ‚Ä¢ Sparsity: 99.74%\n",
      "\n",
      "üìÇ Data Split (80-20):\n",
      "   ‚Ä¢ Training set: 20,000,076 ratings (80%)\n",
      "   ‚Ä¢ Test set: 5,000,019 ratings (20%)\n",
      "   ‚Ä¢ Batch size: 128\n",
      "   ‚Ä¢ Mean rating: 3.53 ¬± 1.06\n",
      "\n",
      "üß† Model Architecture:\n",
      "   ‚Ä¢ Embedding dimension: 50\n",
      "   ‚Ä¢ Total parameters: 11,300,989\n",
      "   ‚Ä¢ Device: cpu\n",
      "\n",
      "üöÄ Training started...\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    101\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     25\u001b[39m recommender.prepare_data(test_size=\u001b[32m0.2\u001b[39m, batch_size=\u001b[32m128\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mrecommender\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[32m     31\u001b[39m recommender.save_model(\u001b[33m'\u001b[39m\u001b[33mmovie_recommender_model.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36mMovieRecommender.train\u001b[39m\u001b[34m(self, epochs, lr, weight_decay, verbose)\u001b[39m\n\u001b[32m    187\u001b[39m     optimizer.zero_grad()\n\u001b[32m    188\u001b[39m     loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     train_loss += loss.item()\n\u001b[32m    193\u001b[39m train_loss /= \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\mrs\\Lib\\site-packages\\torch\\optim\\optimizer.py:526\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    521\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    522\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    523\u001b[39m             )\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# pyrefly: ignore [invalid-param-spec]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\mrs\\Lib\\site-packages\\torch\\optim\\optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\mrs\\Lib\\site-packages\\torch\\optim\\adam.py:248\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    236\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    238\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    239\u001b[39m         group,\n\u001b[32m    240\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m         state_steps,\n\u001b[32m    246\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\mrs\\Lib\\site-packages\\torch\\optim\\optimizer.py:151\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\mrs\\Lib\\site-packages\\torch\\optim\\adam.py:970\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    968\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\mrs\\Lib\\site-packages\\torch\\optim\\adam.py:476\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    472\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(\n\u001b[32m    473\u001b[39m             grad, grad, value=cast(\u001b[38;5;28mfloat\u001b[39m, \u001b[32m1\u001b[39m - beta2)\n\u001b[32m    474\u001b[39m         )\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    479\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function demonstrating the complete recommendation workflow\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\" \" * 20 + \"PYTORCH MOVIE RECOMMENDATION SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # File paths - UPDATE THESE TO YOUR CSV FILE LOCATIONS\n",
    "    RATINGS_PATH = 'dataset/ratings.csv'\n",
    "    MOVIES_PATH = 'dataset/movies_encoded.csv'  # Optional\n",
    "    \n",
    "    # Initialize recommender\n",
    "    recommender = MovieRecommender(\n",
    "        ratings_path=RATINGS_PATH,\n",
    "        movies_path=MOVIES_PATH,\n",
    "        embedding_dim=50\n",
    "    )\n",
    "    \n",
    "    # Prepare data with 80-20 split\n",
    "    recommender.prepare_data(test_size=0.2, batch_size=128, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    recommender.train(epochs=20, lr=0.01, weight_decay=1e-5)\n",
    "    \n",
    "    # Save the trained model\n",
    "    recommender.save_model('movie_recommender_model.pth')\n",
    "    \n",
    "    # DEMONSTRATION: Get recommendations for sample users\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GENERATING TOP-3 RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get sample users\n",
    "    sample_users = recommender.ratings_df['userId'].unique()[:3]\n",
    "    \n",
    "    for user_id in sample_users:\n",
    "        print(f\"\\n{'‚îÄ' * 80}\")\n",
    "        \n",
    "        # Show user profile\n",
    "        profile = recommender.get_user_profile(user_id)\n",
    "        print(f\"üë§ USER {user_id} PROFILE:\")\n",
    "        print(f\"   ‚Ä¢ Total ratings: {profile['num_ratings']}\")\n",
    "        print(f\"   ‚Ä¢ Average rating: {profile['mean_rating']:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Rating range: [{profile['min_rating']:.1f} - {profile['max_rating']:.1f}]\")\n",
    "        \n",
    "        # Get top 3 recommendations\n",
    "        print(f\"\\nüé¨ TOP 3 RECOMMENDATIONS:\")\n",
    "        recommendations = recommender.recommend_top_3(user_id, exclude_rated=True)\n",
    "        \n",
    "        if recommendations is not None:\n",
    "            for _, row in recommendations.iterrows():\n",
    "                print(f\"   #{row['rank']}. Movie {row['movieId']} - \"\n",
    "                      f\"Predicted Rating: {row['predicted_rating']:.2f}\")\n",
    "    \n",
    "    # INTERACTIVE MODE\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"INTERACTIVE RECOMMENDATION MODE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nüí° Enter a user ID to get personalized recommendations\")\n",
    "    print(\"   Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"Enter User ID: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"\\nüëã Thank you for using the Movie Recommender!\")\n",
    "                break\n",
    "            \n",
    "            user_id = int(user_input)\n",
    "            \n",
    "            # Check if user exists\n",
    "            profile = recommender.get_user_profile(user_id)\n",
    "            if profile is None:\n",
    "                print(f\"‚ùå User {user_id} not found. Try another ID.\\n\")\n",
    "                continue\n",
    "            \n",
    "            # Show profile\n",
    "            print(f\"\\nüë§ USER {user_id}:\")\n",
    "            print(f\"   Rated {profile['num_ratings']} movies (avg: {profile['mean_rating']:.2f})\")\n",
    "            \n",
    "            # Get recommendations\n",
    "            print(f\"\\nüé¨ TOP 3 MOVIE RECOMMENDATIONS:\\n\")\n",
    "            recommendations = recommender.recommend_top_3(user_id)\n",
    "            \n",
    "            if recommendations is not None:\n",
    "                print(recommendations.to_string(index=False))\n",
    "            print()\n",
    "            \n",
    "        except ValueError:\n",
    "            print(\"‚ö† Please enter a valid number or 'quit'\\n\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2d925-5fe8-4524-9fcb-3eb9fb290c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
